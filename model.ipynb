{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Assignment                                                                                    \n",
    "                                                                                           Michele Giovanni Calvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all values of proteins for LabelEncoder\n",
    "protein = {\"TTT\" : \"F\", \"CTT\" : \"L\", \"ATT\" : \"I\", \"GTT\" : \"V\",\n",
    "           \"TTC\" : \"F\", \"CTC\" : \"L\", \"ATC\" : \"I\", \"GTC\" : \"V\",\n",
    "           \"TTA\" : \"L\", \"CTA\" : \"L\", \"ATA\" : \"I\", \"GTA\" : \"V\",\n",
    "           \"TTG\" : \"L\", \"CTG\" : \"L\", \"ATG\" : \"M\", \"GTG\" : \"V\",\n",
    "           \"TCT\" : \"S\", \"CCT\" : \"P\", \"ACT\" : \"T\", \"GCT\" : \"A\",\n",
    "           \"TCC\" : \"S\", \"CCC\" : \"P\", \"ACC\" : \"T\", \"GCC\" : \"A\",\n",
    "           \"TCA\" : \"S\", \"CCA\" : \"P\", \"ACA\" : \"T\", \"GCA\" : \"A\",\n",
    "           \"TCG\" : \"S\", \"CCG\" : \"P\", \"ACG\" : \"T\", \"GCG\" : \"A\",\n",
    "           \"TAT\" : \"Y\", \"CAT\" : \"H\", \"AAT\" : \"N\", \"GAT\" : \"D\",\n",
    "           \"TAC\" : \"Y\", \"CAC\" : \"H\", \"AAC\" : \"N\", \"GAC\" : \"D\",\n",
    "           \"TAA\" : \"STOP\", \"CAA\" : \"Q\", \"AAA\" : \"K\", \"GAA\" : \"E\",\n",
    "           \"TAG\" : \"STOP\", \"CAG\" : \"Q\", \"AAG\" : \"K\", \"GAG\" : \"E\",\n",
    "           \"TGT\" : \"C\", \"CGT\" : \"R\", \"AGT\" : \"S\", \"GGT\" : \"G\",\n",
    "           \"TGC\" : \"C\", \"CGC\" : \"R\", \"AGC\" : \"S\", \"GGC\" : \"G\",\n",
    "           \"TGA\" : \"STOP\", \"CGA\" : \"R\", \"AGA\" : \"R\", \"GGA\" : \"G\",\n",
    "           \"TGG\" : \"W\", \"CGG\" : \"R\", \"AGG\" : \"R\", \"GGG\" : \"G\" \n",
    "           }\n",
    "values = set(protein.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts the DNA into proteins. As the hint suggests, it improves the NN performance. \n",
    "def translate(dna, protein):  # from https://www.geeksforgeeks.org/dna-protein-python-3/\n",
    "\n",
    "    protein_sequence = \"\"\n",
    "\n",
    "    # Generate protein sequence\n",
    "    for i in range(0, len(dna)-(3+len(dna)%3), 3):\n",
    "        if protein[dna[i:i+3]] == \"STOP\" :\n",
    "            break\n",
    "        protein_sequence += protein[dna[i:i+3]]\n",
    "    return protein_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"dataset.npy\").item()\n",
    "xs = data[\"genes\"] # [n_sample, arbitrary length string object]\n",
    "ys = data[\"resistant\"] # [n_sample, bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building dataframe\n",
    "data = {\"DNA\": xs, \"Label\":ys}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DNA\"] = df[\"DNA\"].apply(lambda x: translate(x,protein)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNA</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 DNA  Label\n",
       "0  MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...  False\n",
       "1  MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...  False\n",
       "2  MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...  False\n",
       "3  MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...  False\n",
       "4  MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...   True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      "DNA      100000 non-null object\n",
      "Label    100000 non-null bool\n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 879.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                      DNA   Label\n",
       " count                                              100000  100000\n",
       " unique                                              23699       2\n",
       " top     MHYRMIHWMMEIDCNGCANNTLSRRWNYDFWHKHVEQVKCYRHNIR...   False\n",
       " freq                                                 8524   50089, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(), df.info() #visualizing type and if exist missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49911"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking that the data is not imbalanced\n",
    "len(df[df[\"Label\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 301)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum = df['DNA'].apply(len).min()\n",
    "maximum = df['DNA'].apply(len).max()\n",
    "minimum, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will solve the problem using an recurrent neural network it is necessary to decide a proper value for the sequence length. The problem with vanilla RNN is that with long sequences they tend to lose information due to the vanish gradient problem. This occurs when the gradient becomes so small that it will have little to no impact on the training of the weights of the neural networks. A solution to this problem would be using LSTMs. They solve this problem by controlling what should be stored and what should be deleted. These controls are called gates. They allow the LSTM to store information for longer sequence problems. \n",
    "\n",
    "For this problem all the sequence of 301  may be used and for the smaller proteins a padding preprocessing may be applied to obtain the same dimensions. However, being computationally expensive for my machine I have decided to use a sequence of 250 and lose some accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DNA\"] = df[\"DNA\"].apply(lambda x: x[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features\n",
    "\n",
    "To explain the encoding process  I will be using the DNA nucleobases even though I will be working with proteins. Note, that the explanation holds for proteins too.\n",
    "\n",
    "The DNA is composed by the letters A, C, G and T. Since they cannot be used to train the model it is necessary to encode them. I could encode them such that A = 0, C = 1, G = 2 and T = 3. However, the model might give more importance to some of the nucleobases over the others. Hence, a one hot encoding will be applied. Of course the same discussion holds for the proteins, instead of having A,C,G,T we have the variables in the dictionary of _def (translate)_\n",
    "\n",
    "This will create a 4 feature vector for every nucleobase, ex: \n",
    "\n",
    "A = [1 0 0 0]\n",
    "\n",
    "C = [0 1 0 0]\n",
    "\n",
    "G = [0 0 1 0]\n",
    "\n",
    "T = [0 0 0 1]\n",
    "\n",
    "\n",
    "However, the last column can be removed. The algorithm will understand that an input of [0 0 0] will be equivalent to T. However, since we will have to do some padding eventually which adds [0 0 0] it is necessary to keep the last column so that the LSTM understands the difference betweent the padded value and T. \n",
    "\n",
    "Hence, padded value = [0 0 0 0] and T = [0 0 0 1]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DNA\"] = df[\"DNA\"].apply(lambda x: np.array(list(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Label encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(list(values))\n",
    "one_hot_fit = encoder.transform(list(values)) #will later fit the one hot encoder so I am sure of taking all values\n",
    "df[\"DNA\"] = df[\"DNA\"].apply(lambda x: encoder.transform(x))\n",
    "df.iloc[1,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:331: DeprecationWarning: Passing 'n_values' is deprecated in version 0.20 and will be removed in 0.22. You can use the 'categories' keyword instead. 'n_values=n' corresponds to 'categories=[range(n)]'.\n",
      "  warnings.warn(msg, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# One Hot encoder \n",
    "df[\"DNA\"] = df[\"DNA\"].apply(lambda x: x.reshape(len(x),1))\n",
    "onehot = OneHotEncoder(sparse=False, dtype=int, n_values=len(values))\n",
    "onehot.fit(one_hot_fit.reshape(-1,1))\n",
    "df[\"DNA\"] = df[\"DNA\"].apply(lambda x: onehot.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying shape\n",
    "df.iloc[1,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNA</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 DNA  Label\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...      0\n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...      0\n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...      0\n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...      0\n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the data\n",
    "df[\"Label\"] = df[\"Label\"].replace([True,False],[1,0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extracting the data from the dataframe and splitting into training and testing data sets\n",
    "x = df.iloc[:,0].values\n",
    "y = df.iloc[:,-1].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 250, 21)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have array of arrays but LSTM need tensor [# of samples, #timesteps, #features]\n",
    "\n",
    "x_tr = []\n",
    "for i in range(len(x_train)):\n",
    "    x_tr.append(x_train[i])\n",
    "x_t = []\n",
    "for i in range(len(x_test)):\n",
    "    x_t.append(x_test[i])\n",
    "    \n",
    "# Padding\n",
    "x_tr = pad_sequences(x_tr) #makes all time steps the same\n",
    "x_t = pad_sequences(x_t)   \n",
    "x_tr.shape                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have that in both training and test set we have a data with the maximum time steps allowed (250). Hence the padding can be done seperately without risking of having two different sequences for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_tr[0:60000,:]\n",
    "y_train = y_train[0:60000]#used 60,000 because slow training on my computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 29860, 0: 30140})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(y_train) # verifying that also downsampling is still balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "\n",
    "batch = 100\n",
    "epoch = 2 #after several trainings I have noticed that the NN converges after roughly 2 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Model - used to understand after how many epochs the NN converges \n",
    "model = Sequential()\n",
    "model.add(LSTM(16,input_shape = x_tr.shape[1:],batch_size = batch))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.2681 - acc: 0.9302\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1926 - acc: 0.9421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x112923cf8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_tr,y_train, batch_size = batch, epochs=epoch,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 11s 539us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18325849369168282, 0.943600001335144]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_t, y_test, batch_size=batch)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Parameters\n",
    "In this section I will be tuning the parameters using a K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(optimizer,neurons):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons,input_shape = x_tr.shape[1:]))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.compile(optimizer = optimizer , loss = \"binary_crossentropy\",metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making model compatible with scikit-learn\n",
    "classifier = KerasClassifier(build_fn = build_model, batch_size = 100, epochs = epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am tuning the number of neurons in the LSTM layer and the type of optimizer. Other tunings can be made, for example the number of epochs, batches, learning rates. Due to time constraints I have decided to opt for the number of neurons and optimizer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = np.random.randint(2,128,3)\n",
    "parameters = [{\"optimizer\": [\"RMSprop\"],\"neurons\": neurons},\n",
    "             {\"optimizer\": [\"Adam\"],\"neurons\": neurons}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 112s 2ms/step - loss: 0.2891 - acc: 0.9262\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 108s 2ms/step - loss: 0.2000 - acc: 0.9412\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 107s 2ms/step - loss: 0.3281 - acc: 0.9101\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 106s 2ms/step - loss: 0.2118 - acc: 0.9393\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 110s 2ms/step - loss: 0.2917 - acc: 0.9112\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 107s 2ms/step - loss: 0.2051 - acc: 0.9391\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 118s 2ms/step - loss: 0.3269 - acc: 0.9027\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 121s 3ms/step - loss: 0.2070 - acc: 0.9400\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 119s 2ms/step - loss: 0.3112 - acc: 0.9107\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 106s 2ms/step - loss: 0.2049 - acc: 0.9415\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 129s 3ms/step - loss: 0.2549 - acc: 0.9251\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 146s 3ms/step - loss: 0.1921 - acc: 0.9407\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 145s 3ms/step - loss: 0.2443 - acc: 0.9264\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 119s 2ms/step - loss: 0.1916 - acc: 0.9401\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 124s 3ms/step - loss: 0.2445 - acc: 0.9279\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 145s 3ms/step - loss: 0.1979 - acc: 0.9387\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 130s 3ms/step - loss: 0.2379 - acc: 0.9308\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.1844 - acc: 0.9423\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 128s 3ms/step - loss: 0.2463 - acc: 0.9259\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 126s 3ms/step - loss: 0.1894 - acc: 0.9416\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 115s 2ms/step - loss: 0.2513 - acc: 0.9266\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 122s 3ms/step - loss: 0.2027 - acc: 0.9399\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 129s 3ms/step - loss: 0.2559 - acc: 0.9240\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 113s 2ms/step - loss: 0.2035 - acc: 0.9390\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 114s 2ms/step - loss: 0.2586 - acc: 0.9214\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 115s 2ms/step - loss: 0.2038 - acc: 0.9377\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 113s 2ms/step - loss: 0.2460 - acc: 0.9268\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 111s 2ms/step - loss: 0.1952 - acc: 0.9412\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 115s 2ms/step - loss: 0.2443 - acc: 0.9285\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 113s 2ms/step - loss: 0.1857 - acc: 0.9424\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 115s 2ms/step - loss: 0.3108 - acc: 0.9069\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 107s 2ms/step - loss: 0.2007 - acc: 0.9416\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 107s 2ms/step - loss: 0.3034 - acc: 0.9108\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 106s 2ms/step - loss: 0.2062 - acc: 0.9401\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 108s 2ms/step - loss: 0.3186 - acc: 0.9097\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 106s 2ms/step - loss: 0.2165 - acc: 0.9385\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 108s 2ms/step - loss: 0.3294 - acc: 0.8985\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 106s 2ms/step - loss: 0.2056 - acc: 0.9421\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 110s 2ms/step - loss: 0.3698 - acc: 0.8914\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 108s 2ms/step - loss: 0.2180 - acc: 0.9415\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 124s 3ms/step - loss: 0.2448 - acc: 0.9340\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 121s 3ms/step - loss: 0.1910 - acc: 0.9432\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 123s 3ms/step - loss: 0.2558 - acc: 0.9246\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 120s 2ms/step - loss: 0.1913 - acc: 0.9418\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 125s 3ms/step - loss: 0.2450 - acc: 0.9307\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 121s 3ms/step - loss: 0.1902 - acc: 0.9416\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 124s 3ms/step - loss: 0.3083 - acc: 0.9101\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 121s 3ms/step - loss: 0.1960 - acc: 0.9433\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 123s 3ms/step - loss: 0.2498 - acc: 0.9291\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 120s 3ms/step - loss: 0.1971 - acc: 0.9420\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 123s 3ms/step - loss: 0.2502 - acc: 0.9294\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 116s 2ms/step - loss: 0.1936 - acc: 0.9430\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2612 - acc: 0.9197\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 118s 2ms/step - loss: 0.1965 - acc: 0.9411\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 116s 2ms/step - loss: 0.2602 - acc: 0.9246\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 115s 2ms/step - loss: 0.2029 - acc: 0.9411\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 115s 2ms/step - loss: 0.2501 - acc: 0.9273\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 111s 2ms/step - loss: 0.2066 - acc: 0.9396\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 116s 2ms/step - loss: 0.2505 - acc: 0.9268\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 116s 2ms/step - loss: 0.1947 - acc: 0.9420\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 169s 3ms/step - loss: 0.2360 - acc: 0.9332\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.1877 - acc: 0.9423\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with the parameters chosen\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters, scoring = 'accuracy', cv = 5) #5-fold CV\n",
    "# Fit on training set\n",
    "grid_search = grid_search.fit(x_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'neurons': 71, 'optimizer': 'Adam'}\n",
      "Best accuracy: 0.9422666666666667\n"
     ]
    }
   ],
   "source": [
    "#Viewing best parameters in Grid Search\n",
    "best_parameter = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_ #best cros validated mean\n",
    "print('Best parameter: ' + str(best_parameter))\n",
    "print('Best accuracy: ' + str(best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model \n",
    "Using parameters from 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = Sequential()\n",
    "final.add(LSTM(best_parameter[\"neurons\"],input_shape = x_tr.shape[1:],batch_size = batch))\n",
    "final.add(Dense(1))\n",
    "final.add(Activation(\"sigmoid\"))\n",
    "final.compile(optimizer = best_parameter[\"optimizer\"], loss = \"binary_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.2420 - acc: 0.9294\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.1867 - acc: 0.9425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aafaa1a58>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choice of 16 neurons. 1,2 too small. 16 converges with less epochs\n",
    "# explainw y sigmoid, loss = binary_cross_entropy\n",
    "best_parameter[\"neurons\"]\n",
    "final.fit(x_tr,y_train, batch_size = batch, epochs=epoch,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 13s 645us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1775519283115864, 0.943600001335144]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = final.evaluate(x_t, y_test, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.943600001335144"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria\n",
    "\n",
    "I have tried several combinations for the architecture of the NN and saw small improvements when increasing the complexity of the architecture. Therefore, I have decided to stick to a simple architecture with one LSTM input layer and one output layer. \n",
    "\n",
    "The very first training model has 16 input neurons because when I using smaller values (1,2,4,8) the training's accuracy was lower. After several trials I found 16 to be the best value. Furthermore, I have opted for a binary crossentropy loss function because this is a binary classification problem. The advantage of this loss function is that it greatly penalizes bad predictions by using a negative log of the probability.  A sigmoid function is used as the output activation function of the NN as it provides the probability of the output belonging to a \"TRUE\" label. A softmax funciton could have been used, but it would require to use two outputs. And this being a binary classification we just need one output probability as the other can be obtained by: \n",
    "\n",
    "P(FALSE) = 1 - P(TRUE). \n",
    "\n",
    "The biggest challenges I have faced in the problem were not related to the architecture but more on how to preprocess the data. Choosing the right number of time steps and encoding all the variables. By using a simple Label encoder the performances of the NN were somewhat simialr to one hot encoding, with the difference that it took longer to train. Hence, before starting the k-fold CV I applied a one hot encoder as it would perform the cross validation in less time. \n",
    "\n",
    "# Comparing Performances\n",
    "\n",
    "Training score - 94.25%\n",
    "\n",
    "5-fold CV score - 94.23%\n",
    "\n",
    "Test score - 94.36%\n",
    "\n",
    "From the three diferent scores it is possible to see that the model doesn't underfit or overfit. The test score is the same as training score, a little bit higher. What causes this might be the fact that there are 23699 (from **Uploading Data**) combinations of proteins. Since we have 100000 samples it is possible to say that every data point is repeated roughly 4 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
